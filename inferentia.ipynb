{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsjhkKhq/8g+rQjq6um/ON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammadhsiavash/Sentiment-analysis-with-NLP/blob/main/inferentia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhHnGYhMNmBk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install bertviz\n",
        "!pip install datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate\n",
        "!pip install huggingface_hub\n",
        "!pip install sentencepiece\n",
        "!pip install shap\n",
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub  import login\n",
        "login()"
      ],
      "metadata": {
        "id": "UDo3T1bPQQdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from transformers import AlbertForSequenceClassification, AlbertTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def create_word_cloud(series):\n",
        "    # Combine all text from the series into a single string\n",
        "    text = ' '.join(series.tolist())\n",
        "\n",
        "    # Load spaCy's English language model\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    # Create a set of spaCy's English stop words\n",
        "    stopwords = set(nlp.Defaults.stop_words)\n",
        "\n",
        "    # Step 4: Create an instance of WordCloud and generate the word cloud\n",
        "    wordcloud = WordCloud(stopwords=stopwords).generate(text)\n",
        "\n",
        "    # Step 6: Display the word cloud using matplotlib\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_mental_health_references(df):\n",
        "  unique_labels = list(df.subreddit.unique())\n",
        "  unique_labels.extend(list(map(lambda x: x.lower(), unique_labels)))\n",
        "  unique_labels.extend(list(map(lambda x: x.upper(), unique_labels)))\n",
        "  unique_labels = set(unique_labels)\n",
        "  pattern = '|'.join(unique_labels)\n",
        "  df.body = df.body.str.replace(pattern, '', regex=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def not_none(example):\n",
        "    return example['body'] is not None\n",
        "\n",
        "def prepare_dataframe(df):\n",
        "  # Concatenate title and body\n",
        "  df['body'] = df.body.fillna('')\n",
        "  df['body'] = df.body.str.cat(df.title, sep=' ')\n",
        "\n",
        "  # Removed deleted posts\n",
        "  df = df[~df.author.str.contains('\\[deleted\\]')]\n",
        "  df = df[~df.body.str.contains('\\[removed\\]')]\n",
        "  df = df[~df.body.str.contains('\\[deleted\\]')]\n",
        "  df = df[~df.body.str.contains('\\[deleted by user\\]')]\n",
        "\n",
        "  # Removed moderador posts\n",
        "  df = df[df.author!='AutoModerator']\n",
        "\n",
        "  return df[['body', 'subreddit']]\n"
      ],
      "metadata": {
        "id": "qUw_j3qaSWX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"solomonk/reddit_mental_health_posts\")\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "df = prepare_dataframe(df).reset_index(drop=True)\n",
        "df = df.pipe(remove_mental_health_references)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset_sampled = dataset.train_test_split(test_size=0.7, seed=42)['train']\n",
        "\n",
        "train_val_test = dataset_sampled.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_val_test['train']\n",
        "test_val_dataset = train_val_test['test']\n",
        "\n",
        "test_val_split = test_val_dataset.train_test_split(test_size=0.5, seed=42)\n",
        "validation_dataset = test_val_split['train']\n",
        "test_dataset = test_val_split['test']"
      ],
      "metadata": {
        "id": "M2T5_c66SlS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(dataset_sampled['subreddit'])"
      ],
      "metadata": {
        "id": "BFjdt3mP46OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "tV7XcHXyTJpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the inferential of the model"
      ],
      "metadata": {
        "id": "7jtCbywPQwYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "model = AlbertForSequenceClassification.from_pretrained(\"Frorozcol/albert_classification_reddit\",  output_attentions=True).cuda()"
      ],
      "metadata": {
        "id": "EE5uGoiXQuJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers-interpret"
      ],
      "metadata": {
        "id": "iFO2eRnQlXKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "cls_explainer = SequenceClassificationExplainer(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    custom_labels =  le.classes_)\n",
        "word_attributions = cls_explainer(test_dataset[\"body\"][1519], class_name=\"OCD\")"
      ],
      "metadata": {
        "id": "qUwQtu3Prakx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_explainer.visualize(\"distilbert_viz.html\")"
      ],
      "metadata": {
        "id": "qdAHutzErp3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_attributions = cls_explainer(test_dataset[\"body\"][2034], class_name=\"ADHD\")\n",
        "cls_explainer.visualize(\"distilbert_viz.html\")"
      ],
      "metadata": {
        "id": "UEx4vf-_5KDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_explainer"
      ],
      "metadata": {
        "id": "GizSATFf7HLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}